{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow diagram:\n",
    "\n",
    "1. Build a basic parser with BS4 and pandas\n",
    "2. Test the parser, make sure it returns the data in the right formats\n",
    "3. Test the data types\n",
    "4. User function profiling tools to assess bottlenecks\n",
    "5. If the parser is complex, use functional decomposition to make it simpler (ask GPT to help you)\n",
    "6. Apply vectorization instead of loops (loop over columns instead of mixed data types or python loops) (ask GPT to help you)\n",
    "7. Use dasks lazy evaluation and instantiation for ops\n",
    "8. Replace BS4 by lxml (ask GPT to help you). lxml is based on C and is lightning fast\n",
    "9. Parse html files in chunks of files if they are small. First chunk into list of list then feed to parsing function. Parsing function should take multiple file paths.\n",
    "\n",
    "file_chunks = list(chunk_files(html_files, chunk_size))\n",
    "delayed_dataframes = [dask.delayed(process_html_files)(parsing_function, chunk) for chunk in file_chunks]\n",
    "10. Pass custom metadata to dask before computing, with optimized data types\n",
    "11. Use chunk insert to SQL, or 'multi'/asynchronous connection\n",
    "\n",
    "## Considerations during optimization\n",
    "\n",
    "- Reducing I/O operations reading files in parallelized chunks\n",
    "- Improving the efficiency of data processing by streamlining functions (reducing overhead) and avoid unncessary operations\n",
    "- Minimize memory usage by using lazy evaluation and optimized data types\n",
    "\n",
    "## Diagram\n",
    "```\n",
    "+------------------+     +------------------+     +-----------------+\n",
    "| HTML Files       |     | Python Script    |     | Database        |\n",
    "| (Local or Cloud) | --> | with lxml & Dask | --> | (e.g., DuckDB)  |\n",
    "+------------------+     +------------------+     +-----------------+\n",
    "         |                          |                       |\n",
    "         | Batched HTML Files       | DataFrames            | SQL Commands\n",
    "         | Processed in Chunks      | Processed and Merged  | Bulk Insert\n",
    "         |                          |                       |\n",
    "+--------v---------+     +----------v------------+    +-----v---------+\n",
    "| HTML File Chunk  |     | Dask Delayed Tasks    |    | Data Insertion|\n",
    "| Reader           |     | (Parallel Processing) |    | (Bulk Method) |\n",
    "+------------------+     +-----------------------+    +---------------+\n",
    "```\n",
    "## Functional overview\n",
    "```\n",
    "+-------------------------+         +--------------------------+\n",
    "| parse_html_files_to_df  |         | extract_listing_info     |\n",
    "| Input: list[str]        |         | Input: lxml.html.Element |\n",
    "| Output: pd.DataFrame    |-------->| Output: dict             |\n",
    "+-------------------------+         +--------------------------+\n",
    "        |                                     |\n",
    "        | DataFrames                          | Dictionary of data\n",
    "        |                                     |\n",
    "+-------v----------+       +-----------------v--------------------+\n",
    "| process_dataframe|       | extract_price                       |\n",
    "| Input: pd.DataFrame      | Input: list[str]                    |\n",
    "| Output: pd.DataFrame     | Output: Optional[int]               |\n",
    "+------------------+       +-------------------------------------+\n",
    "        |\n",
    "        | Processed DataFrames\n",
    "        |\n",
    "+-------v---------+\n",
    "| Database Insertion (to_sql)  |\n",
    "| Input: pd.DataFrame          |\n",
    "| Output: None                 |\n",
    "+-----------------------------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import regex as re\n",
    "import os\n",
    "import duckdb\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import line_profiler\n",
    "from duck_handler import DatabaseConnection\n",
    "from dask.diagnostics import ProgressBar\n",
    "%load_ext line_profiler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic parsing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html_to_dataframe(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        listings = []\n",
    "\n",
    "        # Extract each article which represents a real estate listing\n",
    "        articles = soup.find_all('article', class_=['item', 'extended-item'])\n",
    "        for article in articles:\n",
    "            # Initialize dictionary to store listing details\n",
    "            listing_info = {}\n",
    "            \n",
    "            # Extracting the title and link of the property\n",
    "            title_link = article.find('a', class_='item-link')\n",
    "            if title_link:\n",
    "                listing_info['title'] = title_link.get('title', '')\n",
    "                # Split the title by space and take the first word as home_type\n",
    "                title_words = listing_info['title'].split()\n",
    "                if title_words:  # Check if there is at least one word\n",
    "                    listing_info['home_type'] = title_words[0]\n",
    "                else:\n",
    "                    listing_info['home_type'] = ''  # Handle cases where the title might be empty\n",
    "                listing_info['link'] = 'https://www.idealista.pt' + title_link.get('href', '')\n",
    "\n",
    "            # Extracting price information\n",
    "            price_info = article.find('span', class_='item-price')\n",
    "            parking_info = article.find('span', class_='item-parking')\n",
    "            if parking_info:\n",
    "                listing_info['garage'] = \"Yes\"\n",
    "            else:\n",
    "                listing_info['garage'] = \"No\"\n",
    "            if price_info:\n",
    "                numeric_price = re.findall(r'\\d+\\.\\d+|\\d+', price_info.text.strip())\n",
    "                if numeric_price:\n",
    "                    listing_info['price'] = numeric_price[0]  \n",
    "\n",
    "            # Extracting detail characters like 'T1', '55 m² área bruta', etc.\n",
    "            details = article.find_all('span', class_='item-detail')\n",
    "            details_text = [detail.get_text(strip=True) for detail in details]\n",
    "            listing_info[\"additional_details\"] = details_text\n",
    "            description = article.find('div', class_=\"item-description description\").text.strip()\n",
    "            listing_info['description'] = description\n",
    "\n",
    "            # Adding the listing to the list\n",
    "            listings.append(listing_info)\n",
    "\n",
    "        # Creating a DataFrame\n",
    "        df = pd.DataFrame(listings)\n",
    "\n",
    "        # Initialize the new columns\n",
    "        df['home_size'] = None\n",
    "        df['home_area'] = None\n",
    "        df['floor'] = None\n",
    "\n",
    "        # Iterate over rows and fill the columns accordingly\n",
    "        for idx, row in df.iterrows():\n",
    "            details = row['additional_details']\n",
    "            if len(details) >= 2:\n",
    "                df.at[idx, 'home_size'] = details[0]\n",
    "                df.at[idx, 'home_area'] = int(re.search(r'\\d+', details[1]).group())\n",
    "                if len(details) >= 3 and 'andar' in details[2]:\n",
    "                    df.at[idx, 'floor'] = details[2]\n",
    "\n",
    "        # Drop the original 'additional_details' column\n",
    "        # df.drop(columns=['additional_details'], inplace=True)\n",
    "        df['elevator'] = df['floor'].str.contains('com elevador')\n",
    "        df['floor'] = df['floor'].apply(lambda x: int(re.search(r'\\d+', str(x)).group()) if x is not None else None)\n",
    "        df['price'] = df['price'].str.replace('.', '')\n",
    "        df['price'] = df['price'].astype(int)\n",
    "        df['price_per_sqr_meter'] = df['price']/df['home_area']\n",
    "        df['elevator'] = df['elevator'].fillna(False)\n",
    "        df['floor'] = df['floor'].fillna(0)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas parsing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_pandas():\n",
    "    directory_path = \"idealista\"\n",
    "\n",
    "    # List HTML files in the directory\n",
    "    html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
    "\n",
    "    # Parse HTML files and create a Dask DataFrame\n",
    "    data = [parse_html_to_dataframe(file_path) for file_path in html_files]\n",
    "    df = pd.concat(data, axis = 0, ignore_index= True)\n",
    "\n",
    "    # Connect to DuckDB\n",
    "    con = duckdb.connect(database='house_prices.db', read_only=False)\n",
    "\n",
    "    # Query to create a DuckDB table from DataFrame\n",
    "    create_table_query = f\"CREATE TABLE html_data AS SELECT * FROM df\"\n",
    "\n",
    "    # Execute the query\n",
    "    con.execute(\"DROP TABLE IF EXISTS html_data\")\n",
    "    con.execute(create_table_query)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Miguel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bs4\\element.py:1247: RuntimeWarning: coroutine 'read_html_files_async' was never awaited\n",
      "  self.parser_class = parser.__class__\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** KeyboardInterrupt exception caught in code being profiled."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 4.78326 s\n",
      "File: C:\\Users\\Miguel\\AppData\\Local\\Temp\\ipykernel_16744\\1234253537.py\n",
      "Function: process_data_pandas at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def process_data_pandas():\n",
      "     2         1          4.0      4.0      0.0      directory_path = \"idealista\"\n",
      "     3                                           \n",
      "     4                                               # List HTML files in the directory\n",
      "     5         1      40263.0  40263.0      0.1      html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
      "     6                                           \n",
      "     7                                               # Parse HTML files and create a Dask DataFrame\n",
      "     8         1   47792357.0    5e+07     99.9      data = [parse_html_to_dataframe(file_path) for file_path in html_files]\n",
      "     9                                               df = pd.concat(data, axis = 0, ignore_index= True)\n",
      "    10                                           \n",
      "    11                                               # Connect to DuckDB\n",
      "    12                                               con = duckdb.connect(database='house_prices.sql', read_only=False)\n",
      "    13                                           \n",
      "    14                                               # Query to create a DuckDB table from DataFrame\n",
      "    15                                               create_table_query = f\"CREATE TABLE html_data AS SELECT * FROM df\"\n",
      "    16                                           \n",
      "    17                                               # Execute the query\n",
      "    18                                               con.execute(\"DROP TABLE IF EXISTS html_data\")\n",
      "    19                                               con.execute(create_table_query)"
     ]
    }
   ],
   "source": [
    "%lprun -f process_data_pandas process_data_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Basic pandas results\n",
    "\n",
    "Total time: 4.78326 s\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def process_data_pandas():\n",
    "     2         1          4.0      4.0      0.0      directory_path = \"idealista\"\n",
    "     3                                           \n",
    "     4                                               # List HTML files in the directory\n",
    "     5         1      40263.0  40263.0      0.1      html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
    "     6                                           \n",
    "     7                                               # Parse HTML files and create a Dask DataFrame\n",
    "     8         1   47792357.0    5e+07     99.9      data = [parse_html_to_dataframe(file_path) for file_path in html_files]\n",
    "     9                                               df = pd.concat(data, axis = 0, ignore_index= True)\n",
    "    10                                           \n",
    "    11                                               # Connect to DuckDB\n",
    "    12                                               con = duckdb.connect(database='house_prices.sql', read_only=False)\n",
    "    13                                           \n",
    "    14                                               # Query to create a DuckDB table from DataFrame\n",
    "    15                                               create_table_query = f\"CREATE TABLE html_data AS SELECT * FROM df\"\n",
    "    16                                           \n",
    "    17                                               # Execute the query\n",
    "    18                                               con.execute(\"DROP TABLE IF EXISTS html_data\")\n",
    "    19                                               con.execute(create_table_query)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(parsing_function, directory_path=\"idealista\"):\n",
    "    # Check connection first\n",
    "    db = DatabaseConnection('house_prices.db')\n",
    "    with db.managed_cursor() as con:\n",
    "        # List HTML files in the directory\n",
    "        html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
    "        \n",
    "        # Use Dask to parse HTML files asynchronously\n",
    "        delayed_dataframes = [dask.delayed(parsing_function)(file) for file in html_files]\n",
    "        with ProgressBar():\n",
    "            # i cant maintain lazy eval because there are some empty dataframes due to parsing inconsistencies\n",
    "            computed_dataframes = dask.compute(*delayed_dataframes, scheduler='processes')\n",
    "\n",
    "        # Filter out empty dataframes\n",
    "        filtered_dataframes = [df for df in computed_dataframes if df is not None and not df.empty]\n",
    "\n",
    "        # Concatenate dataframes efficiently\n",
    "        if filtered_dataframes:\n",
    "            pandas_df = pd.concat(filtered_dataframes, axis=0, ignore_index=True)\n",
    "\n",
    "            # Insert into DuckDB\n",
    "            con.execute(f\"DROP TABLE IF EXISTS {directory_path}\")\n",
    "            pandas_df.to_sql(directory_path, con, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 16.59 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Miguel\\AppData\\Local\\Temp\\ipykernel_16744\\1858589816.py:23: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  pandas_df.to_sql(directory_path, con, index=False, if_exists='replace')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 257.473 s\n",
      "File: C:\\Users\\Miguel\\AppData\\Local\\Temp\\ipykernel_16744\\1858589816.py\n",
      "Function: process_data at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def process_data(parsing_function, directory_path=\"idealista\"):\n",
      "     2                                               # Check connection first\n",
      "     3         1         33.0     33.0      0.0      db = DatabaseConnection('house_prices.db')\n",
      "     4         2    2430868.0    1e+06      0.1      with db.managed_cursor() as con:\n",
      "     5                                                   # List HTML files in the directory\n",
      "     6         1      44211.0  44211.0      0.0          html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
      "     7                                                   \n",
      "     8                                                   # Use Dask to parse HTML files asynchronously\n",
      "     9         1     320066.0 320066.0      0.0          delayed_dataframes = [dask.delayed(parsing_function)(file) for file in html_files]\n",
      "    10         2        394.0    197.0      0.0          with ProgressBar():\n",
      "    11                                                       # i cant maintain lazy eval because there are some empty dataframes due to parsing inconsistencies\n",
      "    12         1  169339803.0    2e+08      6.6              computed_dataframes = dask.compute(*delayed_dataframes, scheduler='processes')\n",
      "    13                                           \n",
      "    14                                                   # Filter out empty dataframes\n",
      "    15         1      27943.0  27943.0      0.0          filtered_dataframes = [df for df in computed_dataframes if df is not None and not df.empty]\n",
      "    16                                           \n",
      "    17                                                   # Concatenate dataframes efficiently\n",
      "    18         1          7.0      7.0      0.0          if filtered_dataframes:\n",
      "    19         1     616761.0 616761.0      0.0              pandas_df = pd.concat(filtered_dataframes, axis=0, ignore_index=True)\n",
      "    20                                           \n",
      "    21                                                       # Insert into DuckDB\n",
      "    22         1     356003.0 356003.0      0.0              con.execute(f\"DROP TABLE IF EXISTS {directory_path}\")\n",
      "    23         1 2401595989.0    2e+09     93.3              pandas_df.to_sql(directory_path, con, index=False, if_exists='replace')"
     ]
    }
   ],
   "source": [
    "%lprun -f process_data process_data(parse_html_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timer unit: 1e-07 s\n",
    "\n",
    "Total time: 257.473 s\n",
    "```\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def process_data(parsing_function, directory_path=\"idealista\"):\n",
    "     2                                               # Check connection first\n",
    "     3         1         33.0     33.0      0.0      db = DatabaseConnection('house_prices.db')\n",
    "     4         2    2430868.0    1e+06      0.1      with db.managed_cursor() as con:\n",
    "     5                                                   # List HTML files in the directory\n",
    "     6         1      44211.0  44211.0      0.0          html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
    "     7                                                   \n",
    "     8                                                   # Use Dask to parse HTML files asynchronously\n",
    "     9         1     320066.0 320066.0      0.0          delayed_dataframes = [dask.delayed(parsing_function)(file) for file in html_files]\n",
    "    10         2        394.0    197.0      0.0          with ProgressBar():\n",
    "    11                                                       # i cant maintain lazy eval because there are some empty dataframes due to parsing inconsistencies\n",
    "    12         1  169339803.0    2e+08      6.6              computed_dataframes = dask.compute(*delayed_dataframes, scheduler='processes')\n",
    "    13                                           \n",
    "    14                                                   # Filter out empty dataframes\n",
    "    15         1      27943.0  27943.0      0.0          filtered_dataframes = [df for df in computed_dataframes if df is not None and not df.empty]\n",
    "    16                                           \n",
    "    17                                                   # Concatenate dataframes efficiently\n",
    "...\n",
    "    19         1     616761.0 616761.0      0.0              pandas_df = pd.concat(filtered_dataframes, axis=0, ignore_index=True)\n",
    "    20                                           \n",
    "    21                                                       # Insert into DuckDB\n",
    "    22         1     356003.0 356003.0      0.0              con.execute(f\"DROP TABLE IF EXISTS {directory_path}\")\n",
    "    23         1 2401595989.0    2e+09     93.3              pandas_df.to_sql(directory_path, con, index=False, if_exists='replace')```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd try\n",
    "\n",
    "Function Decomposition: Break down the process_data function into smaller, more manageable functions. This can improve readability and potentially uncover more opportunities for optimization.\n",
    "\n",
    "\n",
    "Adjusting the chunk sizes in Dask processing can indeed be beneficial if your HTML files are relatively small. By batching these files into larger tasks, you reduce the overhead that comes from managing many small tasks. This change can improve performance by optimizing how work is distributed across processors and reducing the scheduling and communication costs in a parallel computing environment.\n",
    "\n",
    "Here's how you can modify your function to implement batching:\n",
    "Steps to Implement Batching:\n",
    "\n",
    "    Group HTML Files: Divide the list of HTML files into batches (chunks) before processing them with Dask. Each batch will contain multiple files that a single task will process.\n",
    "    Modify the Dask Task: Change the Dask delayed function to accept a list of HTML files instead of a single file and modify the parsing function accordingly if necessary.\n",
    "    Process Batches in Parallel: Use Dask to process each batch of files in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiofiles\n",
    "\n",
    "async def read_html_files_async(directory_path):\n",
    "    html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
    "    return html_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_floor(details):\n",
    "    if len(details) >= 3 and 'andar' in details[2]:\n",
    "        return details[2]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df):\n",
    "    df['home_type'] = df['title'].apply(lambda x: x.split()[0] if x else '')\n",
    "    df['home_size'] = df['additional_details'].apply(lambda x: x[0] if len(x) >= 1 else None)\n",
    "    df['home_area'] = df['additional_details'].apply(lambda x: int(re.search(r'\\d+', x[1]).group()) if len(x) >= 2 else None)\n",
    "    df['floor'] = df['additional_details'].apply(extract_floor)\n",
    "\n",
    "    df['elevator'] = df['floor'].apply(lambda x: 'com elevador' in str(x))\n",
    "    df['floor'] = df['floor'].apply(lambda x: int(re.search(r'\\d+', str(x)).group()) if x and re.search(r'\\d+', str(x)) else 0)\n",
    "\n",
    "    df['price_per_sqr_meter'] = df['price'] / df['home_area']\n",
    "    df.drop(columns=['additional_details'], inplace=True)\n",
    "    df.fillna({'elevator': False, 'floor': 0}, inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_price(article):\n",
    "    price_info = article.find('span', class_='item-price')\n",
    "    if price_info:\n",
    "        numeric_price = re.findall(r'\\d+', price_info.text.replace('.', ''))\n",
    "        return int(numeric_price[0]) if numeric_price else None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_listing_info(article):\n",
    "    listing_info = {\n",
    "        'title': (title_link := article.find('a', class_='item-link')) and title_link.get('title', ''),\n",
    "        'link': 'https://www.idealista.pt' + (title_link.get('href', '') if title_link else ''),\n",
    "        'garage': 'Yes' if article.find('span', class_='item-parking') else 'No',\n",
    "        'price': extract_price(article),\n",
    "        'description': article.find('div', class_=\"item-description description\").text.strip(),\n",
    "        'additional_details': [detail.get_text(strip=True) for detail in article.find_all('span', class_='item-detail')]\n",
    "    }\n",
    "    return listing_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html_to_dataframe(file_path):\n",
    "    # Read the HTML file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "    listings = []\n",
    "    articles = soup.find_all('article', class_=['item', 'extended-item'])\n",
    "    for article in articles:\n",
    "        listing_info = extract_listing_info(article)\n",
    "        listings.append(listing_info)\n",
    "\n",
    "    df = pd.DataFrame(listings)\n",
    "    if not df.empty:\n",
    "        df = process_dataframe(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_files(file_list, chunk_size):\n",
    "    # Generator that yields chunks of files\n",
    "    for i in range(0, len(file_list), chunk_size):\n",
    "        yield file_list[i:i + chunk_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_html_files(parsing_function, files):\n",
    "    # This function processes a batch of HTML files and returns a DataFrame\n",
    "    dataframes = []\n",
    "    for file in files:\n",
    "        df = parsing_function(file)\n",
    "        if df is not None and not df.empty:\n",
    "            dataframes.append(df)\n",
    "    if dataframes:\n",
    "        return pd.concat(dataframes, axis=0, ignore_index=True)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(parsing_function, directory_path=\"idealista\", chunk_size=20):\n",
    "    db = DatabaseConnection('house_prices.db')\n",
    "    with db.managed_cursor() as con:\n",
    "        # List HTML files in the directory\n",
    "        html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
    "\n",
    "        # Create chunks of HTML files\n",
    "        file_chunks = list(chunk_files(html_files, chunk_size))\n",
    "\n",
    "        # Use Dask to parse HTML files in batches\n",
    "        delayed_dataframes = [dask.delayed(process_html_files)(parsing_function, chunk) for chunk in file_chunks]\n",
    "        with ProgressBar():\n",
    "            computed_dataframes = dask.compute(*delayed_dataframes, scheduler='processes')\n",
    "\n",
    "        # Filter out None results from batches\n",
    "        filtered_dataframes = [df for df in computed_dataframes if df is not None]\n",
    "\n",
    "        # Concatenate dataframes efficiently\n",
    "        if filtered_dataframes:\n",
    "            final_df = pd.concat(filtered_dataframes, axis=0, ignore_index=True)\n",
    "\n",
    "            # Insert into database\n",
    "            con.execute(f\"DROP TABLE IF EXISTS {directory_path}\")\n",
    "            final_df.to_sql(directory_path, con, index=False, if_exists='replace', method='multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 16.27 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Miguel\\AppData\\Local\\Temp\\ipykernel_16744\\1981086236.py:24: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  final_df.to_sql(directory_path, con, index=False, if_exists='replace')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 228.601 s\n",
      "File: C:\\Users\\Miguel\\AppData\\Local\\Temp\\ipykernel_16744\\1981086236.py\n",
      "Function: process_data at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def process_data(parsing_function, directory_path=\"idealista\", chunk_size=10):\n",
      "     2         1         50.0     50.0      0.0      db = DatabaseConnection('house_prices.db')\n",
      "     3         2    2785587.0    1e+06      0.1      with db.managed_cursor() as con:\n",
      "     4                                                   # List HTML files in the directory\n",
      "     5         1      47898.0  47898.0      0.0          html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
      "     6                                           \n",
      "     7                                                   # Create chunks of HTML files\n",
      "     8         1        368.0    368.0      0.0          file_chunks = list(chunk_files(html_files, chunk_size))\n",
      "     9                                           \n",
      "    10                                                   # Use Dask to parse HTML files in batches\n",
      "    11         1      68009.0  68009.0      0.0          delayed_dataframes = [dask.delayed(process_html_files)(parsing_function, chunk) for chunk in file_chunks]\n",
      "    12         2        386.0    193.0      0.0          with ProgressBar():\n",
      "    13         1  164562665.0    2e+08      7.2              computed_dataframes = dask.compute(*delayed_dataframes, scheduler='processes')\n",
      "    14                                           \n",
      "    15                                                   # Filter out None results from batches\n",
      "    16         1         86.0     86.0      0.0          filtered_dataframes = [df for df in computed_dataframes if df is not None]\n",
      "    17                                           \n",
      "    18                                                   # Concatenate dataframes efficiently\n",
      "    19         1          3.0      3.0      0.0          if filtered_dataframes:\n",
      "    20         1      69317.0  69317.0      0.0              final_df = pd.concat(filtered_dataframes, axis=0, ignore_index=True)\n",
      "    21                                           \n",
      "    22                                                       # Insert into database\n",
      "    23         1     328203.0 328203.0      0.0              con.execute(f\"DROP TABLE IF EXISTS {directory_path}\")\n",
      "    24         1 2118142445.0    2e+09     92.7              final_df.to_sql(directory_path, con, index=False, if_exists='replace')"
     ]
    }
   ],
   "source": [
    "%lprun -f process_data process_data(parse_html_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dask + functional decomposition\n",
    "\n",
    "```\n",
    "Timer unit: 1e-07 s\n",
    "\n",
    "Total time: 228.601 s\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def process_data(parsing_function, directory_path=\"idealista\", chunk_size=10):\n",
    "     2         1         50.0     50.0      0.0      db = DatabaseConnection('house_prices.db')\n",
    "     3         2    2785587.0    1e+06      0.1      with db.managed_cursor() as con:\n",
    "     4                                                   # List HTML files in the directory\n",
    "     5         1      47898.0  47898.0      0.0          html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
    "     6                                           \n",
    "     7                                                   # Create chunks of HTML files\n",
    "     8         1        368.0    368.0      0.0          file_chunks = list(chunk_files(html_files, chunk_size))\n",
    "     9                                           \n",
    "    10                                                   # Use Dask to parse HTML files in batches\n",
    "    11         1      68009.0  68009.0      0.0          delayed_dataframes = [dask.delayed(process_html_files)(parsing_function, chunk) for chunk in file_chunks]\n",
    "    12         2        386.0    193.0      0.0          with ProgressBar():\n",
    "    13         1  164562665.0    2e+08      7.2              computed_dataframes = dask.compute(*delayed_dataframes, scheduler='processes')\n",
    "    14                                           \n",
    "    15                                                   # Filter out None results from batches\n",
    "    16         1         86.0     86.0      0.0          filtered_dataframes = [df for df in computed_dataframes if df is not None]\n",
    "    17                                           \n",
    "...\n",
    "    20         1      69317.0  69317.0      0.0              final_df = pd.concat(filtered_dataframes, axis=0, ignore_index=True)\n",
    "    21                                           \n",
    "    22                                                       # Insert into database\n",
    "    23         1     328203.0 328203.0      0.0              con.execute(f\"DROP TABLE IF EXISTS {directory_path}\")\n",
    "    24         1 2118142445.0    2e+09     92.7              final_df.to_sql(directory_path, con, index=False, if_exists='replace')```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "\n",
    "def parse_html_files_to_dataframe(file_paths):\n",
    "    listings = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Read and parse each HTML file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            parsed_html = html.parse(file)\n",
    "\n",
    "        # Extract articles that represent real estate listings\n",
    "        articles = parsed_html.xpath('//article[contains(@class, \"item\") or contains(@class, \"extended-item\")]')\n",
    "        for article in articles:\n",
    "            listing_info = extract_listing_info(article)\n",
    "            listings.append(listing_info)\n",
    "\n",
    "    # Create a DataFrame from all collected listings\n",
    "    df = pd.DataFrame(listings)\n",
    "    if not df.empty:\n",
    "        df = process_dataframe(df)\n",
    "    return df\n",
    "\n",
    "def extract_listing_info(article):\n",
    "    # Extracts data from an article and returns a dictionary of listing info\n",
    "    return {\n",
    "        'title': article.xpath('.//a[@class=\"item-link\"]/@title')[0] if article.xpath('.//a[@class=\"item-link\"]/@title') else '',\n",
    "        'link': 'https://www.idealista.pt' + (article.xpath('.//a[@class=\"item-link\"]/@href')[0] if article.xpath('.//a[@class=\"item-link\"]/@href') else ''),\n",
    "        'description': article.xpath('.//div[@class=\"item-description description\"]/text()')[0].strip() if article.xpath('.//div[@class=\"item-description description\"]/text()') else '',\n",
    "        'garage': \"Yes\" if article.xpath('.//span[@class=\"item-parking\"]') else \"No\",\n",
    "        'price': extract_price(article.xpath('.//span[@class=\"item-price\"]/text()')),\n",
    "        'additional_details': article.xpath('.//span[@class=\"item-detail\"]/text()')\n",
    "    }\n",
    "\n",
    "def extract_price(price_texts):\n",
    "    if price_texts:\n",
    "        numeric_price = re.findall(r'\\d+', price_texts[0].replace('.', ''))\n",
    "        return int(numeric_price[0]) if numeric_price else None\n",
    "    return None\n",
    "\n",
    "def process_dataframe(df):\n",
    "    df['home_size'] = df['additional_details'].apply(lambda x: x[0] if x else None)\n",
    "    df['home_area'] = df['additional_details'].apply(lambda x: int(re.search(r'\\d+', x[1]).group()) if len(x) >= 2 else None)\n",
    "    df['floor'] = df['additional_details'].apply(lambda x: x[2] if len(x) >= 3 and 'andar' in x[2] else None)\n",
    "    df['elevator'] = df['floor'].apply(lambda x: 'com elevador' in str(x) if x else False)\n",
    "    df['floor'] = df['floor'].apply(lambda x: int(re.search(r'\\d+', str(x)).group()) if x else 0)\n",
    "    df['price_per_sqr_meter'] = df['price'] / df['home_area']\n",
    "    df.drop(columns=['additional_details'], inplace=True)\n",
    "    df.fillna({'elevator': False, 'floor': 0}, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(parsing_function, directory_path=\"idealista\", chunk_size=10):\n",
    "    db = DatabaseConnection('house_prices.db')\n",
    "    with db.managed_cursor() as con:\n",
    "        # List HTML files in the directory\n",
    "        html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
    "\n",
    "        # Create chunks of HTML files\n",
    "        file_chunks = list(chunk_files(html_files, chunk_size))\n",
    "\n",
    "        # Use Dask to parse HTML files in batches\n",
    "        delayed_dataframes = [dask.delayed(process_html_files)(parsing_function, chunk) for chunk in file_chunks]\n",
    "        with ProgressBar():\n",
    "            computed_dataframes = dask.compute(*delayed_dataframes, \n",
    "                                               scheduler='processes')\n",
    "\n",
    "        # Filter out None results from batches\n",
    "        filtered_dataframes = [df for df in computed_dataframes if df is not None]\n",
    "\n",
    "        # Concatenate dataframes efficiently\n",
    "        if filtered_dataframes:\n",
    "            final_df = pd.concat(filtered_dataframes, axis=0, ignore_index=True)\n",
    "\n",
    "            # Insert into database\n",
    "            con.execute(f\"DROP TABLE IF EXISTS {directory_path}\")\n",
    "            final_df.to_sql(directory_path, con, index=False, if_exists='replace', method='multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_html_files(parsing_function, files):\n",
    "    # This function processes a batch of HTML files and returns a DataFrame\n",
    "    dataframes = []\n",
    "    for file in files:\n",
    "        df = parsing_function(file)\n",
    "        if df is not None and not df.empty:\n",
    "            dataframes.append(df)\n",
    "    if dataframes:\n",
    "        return pd.concat(dataframes, axis=0, ignore_index=True)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 277\n",
      "Chunk size: 10\n",
      "Number of chunks: 28\n",
      "Chunk 1: 10 files\n",
      "Chunk 2: 10 files\n",
      "Chunk 3: 10 files\n",
      "Chunk 4: 10 files\n",
      "Chunk 5: 10 files\n",
      "Chunk 6: 10 files\n",
      "Chunk 7: 10 files\n",
      "Chunk 8: 10 files\n",
      "Chunk 9: 10 files\n",
      "Chunk 10: 10 files\n",
      "Chunk 11: 10 files\n",
      "Chunk 12: 10 files\n",
      "Chunk 13: 10 files\n",
      "Chunk 14: 10 files\n",
      "Chunk 15: 10 files\n",
      "Chunk 16: 10 files\n",
      "Chunk 17: 10 files\n",
      "Chunk 18: 10 files\n",
      "Chunk 19: 10 files\n",
      "Chunk 20: 10 files\n",
      "Chunk 21: 10 files\n",
      "Chunk 22: 10 files\n",
      "Chunk 23: 10 files\n",
      "Chunk 24: 10 files\n",
      "Chunk 25: 10 files\n",
      "Chunk 26: 10 files\n",
      "Chunk 27: 10 files\n",
      "Chunk 28: 7 files\n",
      "[                                        ] | 0% Completed | 587.00 us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 19.93 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Miguel\\AppData\\Local\\Temp\\ipykernel_16744\\1320541976.py:30: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  final_df.to_sql(directory_path, con, index=False, if_exists='replace', method='multi')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 22.1248 s\n",
      "File: C:\\Users\\Miguel\\AppData\\Local\\Temp\\ipykernel_16744\\1320541976.py\n",
      "Function: process_data at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def process_data(parsing_function, directory_path=\"idealista\", chunk_size=10):\n",
      "     2         1         41.0     41.0      0.0      db = DatabaseConnection('house_prices.db')\n",
      "     3         2    2080322.0    1e+06      0.9      with db.managed_cursor() as con:\n",
      "     4                                                   # List HTML files in the directory\n",
      "     5         1      76743.0  76743.0      0.0          html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
      "     6                                           \n",
      "     7                                                   # Create chunks of HTML files\n",
      "     8         1        662.0    662.0      0.0          file_chunks = list(chunk_files(html_files, chunk_size))\n",
      "     9         1       3587.0   3587.0      0.0          print(f\"Total files: {len(html_files)}\")\n",
      "    10         1        160.0    160.0      0.0          print(f\"Chunk size: {chunk_size}\")\n",
      "    11         1        139.0    139.0      0.0          print(f\"Number of chunks: {len(file_chunks)}\")\n",
      "    12        29        198.0      6.8      0.0          for idx, chunk in enumerate(file_chunks):\n",
      "    13        28       6534.0    233.4      0.0              print(f\"Chunk {idx + 1}: {len(chunk)} files\")\n",
      "    14                                           \n",
      "    15                                                   # Use Dask to parse HTML files in batches\n",
      "    16         1      97062.0  97062.0      0.0          delayed_dataframes = [dask.delayed(process_html_files)(parsing_function, chunk) for chunk in file_chunks]\n",
      "    17         2        521.0    260.5      0.0          with ProgressBar():\n",
      "    18         2  201413020.0    1e+08     91.0              computed_dataframes = dask.compute(*delayed_dataframes, \n",
      "    19         1          5.0      5.0      0.0                                                 scheduler='processes')\n",
      "    20                                           \n",
      "    21                                                   # Filter out None results from batches\n",
      "    22         1        101.0    101.0      0.0          filtered_dataframes = [df for df in computed_dataframes if df is not None]\n",
      "    23                                           \n",
      "    24                                                   # Concatenate dataframes efficiently\n",
      "    25         1          3.0      3.0      0.0          if filtered_dataframes:\n",
      "    26         1      81922.0  81922.0      0.0              final_df = pd.concat(filtered_dataframes, axis=0, ignore_index=True)\n",
      "    27                                           \n",
      "    28                                                       # Insert into database\n",
      "    29         1     458093.0 458093.0      0.2              con.execute(f\"DROP TABLE IF EXISTS {directory_path}\")\n",
      "    30         1   17028626.0    2e+07      7.7              final_df.to_sql(directory_path, con, index=False, if_exists='replace', method='multi')"
     ]
    }
   ],
   "source": [
    "%lprun -f process_data process_data(parse_html_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking + lxml\n",
    "\n",
    "```\n",
    "Timer unit: 1e-07 s\n",
    "\n",
    "Total time: 22.1248 s\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def process_data(parsing_function, directory_path=\"idealista\", chunk_size=10):\n",
    "     2         1         41.0     41.0      0.0      db = DatabaseConnection('house_prices.db')\n",
    "     3         2    2080322.0    1e+06      0.9      with db.managed_cursor() as con:\n",
    "     4                                                   # List HTML files in the directory\n",
    "     5         1      76743.0  76743.0      0.0          html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
    "     6                                           \n",
    "     7                                                   # Create chunks of HTML files\n",
    "     8         1        662.0    662.0      0.0          file_chunks = list(chunk_files(html_files, chunk_size))\n",
    "     9         1       3587.0   3587.0      0.0          print(f\"Total files: {len(html_files)}\")\n",
    "    10         1        160.0    160.0      0.0          print(f\"Chunk size: {chunk_size}\")\n",
    "    11         1        139.0    139.0      0.0          print(f\"Number of chunks: {len(file_chunks)}\")\n",
    "    12        29        198.0      6.8      0.0          for idx, chunk in enumerate(file_chunks):\n",
    "    13        28       6534.0    233.4      0.0              print(f\"Chunk {idx + 1}: {len(chunk)} files\")\n",
    "    14                                           \n",
    "    15                                                   # Use Dask to parse HTML files in batches\n",
    "    16         1      97062.0  97062.0      0.0          delayed_dataframes = [dask.delayed(process_html_files)(parsing_function, chunk) for chunk in file_chunks]\n",
    "    17         2        521.0    260.5      0.0          with ProgressBar():\n",
    "...\n",
    "    26         1      81922.0  81922.0      0.0              final_df = pd.concat(filtered_dataframes, axis=0, ignore_index=True)\n",
    "    27                                           \n",
    "    28                                                       # Insert into database\n",
    "    29         1     458093.0 458093.0      0.2              con.execute(f\"DROP TABLE IF EXISTS {directory_path}\")\n",
    "    30         1   17028626.0    2e+07      7.7              final_df.to_sql(directory_path, con, index=False, if_exists='replace', method='multi')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
